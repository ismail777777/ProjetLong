{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['APPLE1', 'BANANA1', 'ORANGE2', 'GRAPE2', 'PINEAPPLE3', 'MELON3']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import itertools\n",
    "# Initialize Spark context\n",
    "conf = SparkConf().setAppName(\"ApplyFunctionToPairRDD\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Example PairRDD\n",
    "pair_rdd = sc.parallelize([(1, [\"apple\", \"banana\"]), (2, [\"orange\", \"grape\"]), (3, [\"pineapple\", \"melon\"])])\n",
    "\n",
    "# Function to apply to each value\n",
    "def process_values(key, strings_list):\n",
    "    # Here key is used as an argument to the function\n",
    "    return [string.upper() + str(key) for string in strings_list]\n",
    "\n",
    "# Apply the function to each value in the PairRDD and remove the key\n",
    "processed_rdd = pair_rdd.flatMap(lambda pair: (process_values(pair[0], pair[1])))\n",
    "#processed_rdd = processed_rdd.map(lambda pair: pair[1])\n",
    "# Print the result\n",
    "print(processed_rdd.collect())\n",
    "\n",
    "# Stop Spark context\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['banana'], ['khrya']]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Initialize Spark context\n",
    "conf = SparkConf().setAppName(\"GetValueByKey\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Example RDD of key-value pairs\n",
    "rdd = sc.parallelize([(1, [\"apple\"]), (2, [\"banana\"]), (3, [\"orange\"]), (2,['khrya'])])\n",
    "\n",
    "# Key to search for\n",
    "key_to_find = 2\n",
    "\n",
    "# Retrieve the value associated with the key using lookup\n",
    "values = rdd.lookup(key_to_find)\n",
    "\n",
    "# Print the value\n",
    "print(values)  # Output: ['banana']\n",
    "\n",
    "# Stop Spark context\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10, 1, 2, 5, 7, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [[2,4,6,8,10], [1,2,5,7,9]]\n",
    "h = [item for sublist in l for item in sublist]\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,2,3', '4,5,6', '7,8,9']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Initialize Spark context\n",
    "conf = SparkConf().setAppName(\"SaveRDDOfLists\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Example RDD of lists\n",
    "rdd = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Specify the output path\n",
    "output_path = \"output_path\"\n",
    "\n",
    "# Save the RDD to a text file\n",
    "zz = rdd.map(lambda lst: ','.join(map(str, lst))).collect()\n",
    "print(zz)\n",
    "# Stop Spark context\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/othmane/Downloads/Prj_Lng/data/cleaned/assembeled/subject/1.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = 'subject'\n",
    "counter =1\n",
    "data_path  = '/home/othmane/Downloads/Prj_Lng/data'\n",
    "path = f\"{data_path}/cleaned/assembeled/{subject}/{counter}.txt\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Initialize Spark context\n",
    "conf = SparkConf().setAppName(\"SaveRDDOfStrings\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Example list of long strings\n",
    "long_strings = [\"long string 1\", \"long string 2\", \"long string 3\", \"hhhhhhhhhhhhhhhhhh\", \"rrrrrrrrrrrr\",\"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\",\"ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\"]\n",
    "\n",
    "# Create an RDD from the list\n",
    "rdd = sc.parallelize(long_strings)\n",
    "\n",
    "# Format each string to be written to a separate line\n",
    "\n",
    "\n",
    "# Specify the output path\n",
    "output_path = \"/home/othmane/Downloads/Prj_Lng/testingtesting\"\n",
    "\n",
    "# Save the RDD to a text file\n",
    "#rdd.coalesce(1).saveAsTextFile(output_path)\n",
    "rdd.saveAsTextFile(output_path)\n",
    "# Stop Spark context\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['long string 1', 'long string 2', 'long string 3', 'hhhhhhhhhhhhhhhhhh', 'rrrrrrrrrrrr', 'yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy', 'ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Initialize Spark context\n",
    "conf = SparkConf().setAppName(\"ReadRDDOfStrings\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Specify the input path where the text files are saved\n",
    "input_path = \"/home/othmane/Downloads/Prj_Lng/testingtesting\"\n",
    "\n",
    "# Read the text files back into Spark as an RDD of strings\n",
    "rdd = sc.textFile(input_path)\n",
    "\n",
    "# Print the result\n",
    "print(rdd.collect())\n",
    "\n",
    "# Stop Spark context\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['555555555555555555555555', '22222222222222222222222222', '77777777777777777777777777777777777777', '444444444444444444444444', '888888888888888888888888888888888888888888888888888', '111111111111111111111111111111111111111111111111111', '6666666666666666666666666666666666666666666666666666666666', '33333333333333333333333333333333333333333333333333333333', '101010101050505050505255251181500202202020020000000000000']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Initialize Spark context\n",
    "conf = SparkConf().setAppName(\"FlattenRDDOfLists\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Example RDD of lists\n",
    "rdd_of_lists = sc.parallelize([['555555555555555555555555', '22222222222222222222222222', '77777777777777777777777777777777777777'], ['444444444444444444444444', '888888888888888888888888888888888888888888888888888', '111111111111111111111111111111111111111111111111111'], ['6666666666666666666666666666666666666666666666666666666666', '33333333333333333333333333333333333333333333333333333333', '101010101050505050505255251181500202202020020000000000000']])\n",
    "\n",
    "# Flatten the RDD\n",
    "flattened_rdd = rdd_of_lists.flatMap(lambda x: x)\n",
    "\n",
    "# Print the result\n",
    "print(flattened_rdd.collect())\n",
    "\n",
    "# Stop Spark context\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'apple'), (1, 'banana'), (1, 'orange'), (1, 'grape'), (1, 'pineapple'), (1, 'melon')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Initialize Spark context\n",
    "conf = SparkConf().setAppName(\"FlatMapWithPairRDD\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create an example Pair RDD\n",
    "pair_rdd = sc.parallelize([([\"apple\", \"banana\"]), ([\"orange\", \"grape\"]), ([\"pineapple\", \"melon\"])])\n",
    "count = 1\n",
    "# Define a function that takes key and value and returns a list\n",
    "def flat_map_function(value):\n",
    "    ret = [(count,val) for val in value]\n",
    "    #count+=1\n",
    "    return ret\n",
    "\n",
    "# Apply flatMap with the function on the Pair RDD\n",
    "flattened_pair_rdd = (pair_rdd.flatMap(lambda k: flat_map_function(k)))\n",
    "#.flatMap(lambda x:x)\n",
    "\n",
    "# Print the result\n",
    "print(flattened_pair_rdd.collect())\n",
    "#output_path = \"/home/othmane/Downloads/Prj_Lng/testingtesting\"\n",
    "\n",
    "# Save the RDD to a text file\n",
    "#rdd.coalesce(1).saveAsTextFile(output_path)\n",
    "#flattened_pair_rdd.saveAsTextFile(output_path)\n",
    "# Stop Spark context\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from os import close\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import ast\n",
    "# Initialize Spark context\n",
    "conf = SparkConf().setAppName(\"ReadRDDOfStrings\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Specify the input path where the text files are saved\n",
    "input_path = \"/home/othmane/Downloads/Prj_Lng/data/cleaned/assembeled/DE_Data_Encryption\"\n",
    "\n",
    "# Read the text files back into Spark as an RDD of strings\n",
    "rdd = sc.textFile(input_path).map(lambda line: ast.literal_eval(line))\n",
    "\n",
    "# Print the result\n",
    "list = rdd.collect()\n",
    "conter =0\n",
    "#for sub in (list):\n",
    "#    for j in range(len(sub)):\n",
    "#        conter+=1\n",
    "#        with open(f'/home/othmane/Downloads/Prj_Lng/{conter}.txt','w') as f:\n",
    "#            f.write(sub[j])\n",
    "#            f.close()\n",
    "# Stop Spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#include <stdio.h>\\ntypedef struct {\\n    int x;\\n    int y;\\n} Point;\\ntypedef struct {\\n    int red;\\n    int green;\\n    int blue;\\n} Color;\\nstruct Complex {\\n    double real;\\n    double imag;\\n};\\nstruct Compilation{\\n    double real;\\n    double imag;\\n};\\nPoint g_point;\\nint g_int;\\nint *g_pointer;\\nvoid printPoint(Point p) {\\n    Complex z;\\n    printf(\"Point: (%d, %d)\\\\n\", p.x, p.y);\\n}\\nvoid printColor(Color c, Point p) {\\n    printPoint(p);\\n    printPoint(point);\\n    g_int = 0;\\n    printf(\"Color: (R:%d, G:%d, B:%d)\\\\n\", c.red, c.green, c.blue);\\n}\\nvoid printComplex(Complex z) {\\n    printf(\"Complex Number: %.2f + %.2fi\\\\n\", z.real, z.imag);\\n}\\nvoid modifyValues(int *ptr) {\\n    printPoint(g_point);\\n    (*ptr)++;\\n}\\nvoid modifyPoint(Point *p, int * ptr) {\\n    modifyPoint(ptr);\\n    p->x++;\\n    p->y++;\\n}\\nvoid printPointAndColor(Point p, Color c) {\\n    printPoint(p);\\n    printColor(c);\\n}\\nint main() {\\n    modifyValues(g_pointer);\\n    int num = 10;\\n    int *ptr = &num;\\n    g_int++;\\n    printf(\"Original Values:\\\\n\");\\n    printPoint(point);\\n    printColor(color);\\n    printComplex(complexNum);\\n    printf(\"Integer value: %d\\\\n\", num);\\n    modifyPoint(&point);\\n    modifyValues(ptr);\\n    printf(\"\\\\nModified Values:\\\\n\");\\n    printPointAndColor(point, color);\\n    printComplex(complexNum);\\n    printf(\"Integer value: %d\\\\n\", num);\\n    return 0;\\n}',\n",
       " '//...\\nstruct Complex {\\n    double real;\\n    double imag;\\n};\\nvoid printComplex(Complex z) {\\n    printf(\"Complex Number: %.2f + %.2fi\\\\n\", z.real, z.imag);\\n}\\n//...',\n",
       " '//...\\ntypedef struct {\\n    int x;\\n    int y;\\n} Point;\\ntypedef struct {\\n    int red;\\n    int green;\\n    int blue;\\n} Color;\\nint g_int;\\n    return 0;\\nvoid printPoint(Point p) {\\n    Complex z;\\n    printf(\"Point: (%d, %d)\\\\n\", p.x, p.y);\\n}\\nvoid printColor(Color c, Point p) {\\n    printPoint(p);\\n    printPoint(point);\\n    g_int = 0;\\n    printf(\"Color: (R:%d, G:%d, B:%d)\\\\n\", c.red, c.green, c.blue);\\n}\\n//...',\n",
       " '//...\\ntypedef struct {\\n    int x;\\n    int y;\\n} Point;\\nPoint g_point;\\n    int *ptr = &num;\\nvoid printPoint(Point p) {\\n    Complex z;\\n    printf(\"Point: (%d, %d)\\\\n\", p.x, p.y);\\n}\\nvoid modifyValues(int *ptr) {\\n    printPoint(g_point);\\n    (*ptr)++;\\n}\\n//...',\n",
       " '//...\\nstruct Complex {\\n    double real;\\n    double imag;\\n};\\ntypedef struct {\\n    int x;\\n    int y;\\n} Point;\\nvoid printPoint(Point p) {\\n    Complex z;\\n    printf(\"Point: (%d, %d)\\\\n\", p.x, p.y);\\n}\\n//...',\n",
       " '//...\\ntypedef struct {\\n    int x;\\n    int y;\\n} Point;\\n    int *ptr = &num;\\nvoid modifyPoint(Point *p, int * ptr) {\\n    modifyPoint(ptr);\\n    p->x++;\\n    p->y++;\\n}\\n//...',\n",
       " '//...\\ntypedef struct {\\n    int x;\\n    int y;\\n} Point;\\ntypedef struct {\\n    int red;\\n    int green;\\n    int blue;\\n} Color;\\nvoid printPoint(Point p) {\\n    Complex z;\\n    printf(\"Point: (%d, %d)\\\\n\", p.x, p.y);\\n}\\nvoid printColor(Color c, Point p) {\\n    printPoint(p);\\n    printPoint(point);\\n    g_int = 0;\\n    printf(\"Color: (R:%d, G:%d, B:%d)\\\\n\", c.red, c.green, c.blue);\\n}\\nvoid printPointAndColor(Point p, Color c) {\\n    printPoint(p);\\n    printColor(c);\\n}\\n//...']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Prompt: Imagine you have an embedded system that monitors temperature and controls a fan based on a predefined temperature threshold. You need to write a code that continuously reads temperatures, turns on the fan if the temperature goes above a certain threshold, and turns off the fan if the temperature falls below the threshold. Additionally, provide a function to control the fan. Write a C program to implement this functionality.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "# Initialize the OpenAI API client\n",
    "openai.api_key = '....'\n",
    "\n",
    "# Function to make a request to the OpenAI API for text generation\n",
    "\n",
    "\n",
    "code = \"\"\"#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <stdbool.h>\n",
    "\n",
    "#define TEMP_THRESHOLD 30\n",
    "#define FAN_PIN 8\n",
    "\n",
    "void read_temperature(int *temperature) {\n",
    "    *temperature = rand() % 50;\n",
    "}\n",
    "\n",
    "void control_fan(bool turn_on) {\n",
    "    if (turn_on) {\n",
    "        printf(\"Fan is turned ON.\\n\");\n",
    "    } else {\n",
    "        printf(\"Fan is turned OFF.\\n\");\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int temperature;\n",
    "    bool fan_on = false;\n",
    "\n",
    "    while (true) {\n",
    "        read_temperature(&temperature);\n",
    "        if (temperature > TEMP_THRESHOLD && !fan_on) {\n",
    "            fan_on = true;\n",
    "            control_fan(true);\n",
    "        } else if (temperature <= TEMP_THRESHOLD && fan_on) {\n",
    "            fan_on = false;\n",
    "            control_fan(false);\n",
    "        }\n",
    "\n",
    "        for (int i = 0; i < 1000000; i++);\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "rqst = \"For the following embedded systems code, give me the prompt i could have given you\\\n",
    "for you to give me that code as a response, in this prompt detail the scenario with respect to embedded systems:\" + \"\\n\" + code\n",
    "response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",  # Use the gpt-3.5-turbo model to automatically annotate each code of the subject\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": rqst}\n",
    "                ],\n",
    "                max_tokens=200  # Specify the maximum length of the generated text\n",
    "            )\n",
    "\n",
    "prompt = response.choices[0].message['content']\n",
    "#prompt = '. '.join(prompt.split('\\n'))\n",
    "# Example usage\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
